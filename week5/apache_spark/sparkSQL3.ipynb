{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "import pyspark.sql.functions as func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "        .master(\"local[*]\")\\\n",
    "        .appName(\"SparkSQL\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "myschema = StructType([\n",
    "    StructField(\"userID\",IntegerType(),True),\n",
    "    StructField(\"name\",StringType(),True),\n",
    "    StructField(\"age\",IntegerType(),True),\n",
    "    StructField(\"friends\",IntegerType(),True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = spark.read.csv(\"fakefriends.csv\",schema=myschema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userID: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- friends: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---+-------+\n",
      "|userID|    name|age|friends|\n",
      "+------+--------+---+-------+\n",
      "|     0|    Will| 33|    385|\n",
      "|     1|Jean-Luc| 26|      2|\n",
      "|     2|    Hugh| 55|    221|\n",
      "|     3|  Deanna| 40|    465|\n",
      "|     4|   Quark| 68|     21|\n",
      "+------+--------+---+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = people.select(people.userID,people.name,people.age,people.friends)\\\n",
    "               .where(people.age < 30)\\\n",
    "               .withColumn('insert_ts',func.current_timestamp())\\\n",
    "               .orderBy(people.userID).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caching - \n",
    " - Store the result of the transformation once its called the first time in RAM - memory - saves computation time\n",
    " - There's also a more customisable option called persist - where you get to choose whether you wanna store the result in RAM or on disk\n",
    " - e.g. output.persist(StorageLevel.MEMORY_AND_DISK) - Telling spark to store this in RAM\n",
    " - output.persist(StorageLevel.DISK_ONLY) - Don't waste RAM - write this to Disk\n",
    " - output.persist(StorageLevel.MEMORY_ONLY_SER) - Keep it in RAM, but store it as serialized objects => saves space, but costs CPU to decompress\n",
    ".cache is a shortcut => its literally the same as .persist(StorageLevel.MEMORY_AND_DISK)\n",
    "=> you can unpersist too..if you're done with a cached/persisted DataFrame => output.unpersist()\n",
    "=> Even better - to uncache immediately, use output.unpersist(blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.createOrReplaceTempView(\"peoples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|userID|    name|\n",
      "+------+--------+\n",
      "|     1|Jean-Luc|\n",
      "|     9|    Hugh|\n",
      "|    16|  Weyoun|\n",
      "|    21|   Miles|\n",
      "|    24|  Julian|\n",
      "|    25|     Ben|\n",
      "|    26|  Julian|\n",
      "|    32|     Nog|\n",
      "|    35| Beverly|\n",
      "|    46|    Morn|\n",
      "|    47|   Brunt|\n",
      "|    48|     Nog|\n",
      "|    52| Beverly|\n",
      "|    54|   Brunt|\n",
      "|    60|  Geordi|\n",
      "|    66|  Geordi|\n",
      "|    72|  Kasidy|\n",
      "|    73|   Brunt|\n",
      "|    84|     Ben|\n",
      "|    89|    Worf|\n",
      "+------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select userID, name from peoples\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "writing the result to disk:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark Writing:\n",
    " - Spark writing is how you get a dataframe out of memory and onto some storage local disk, HDFS, S3, whatever. Its not just dumping data - its a distributed process, so spark gives you a pile of knobs to control how, where, and what gets written\n",
    "\n",
    " - output.write => Starts the writing process, for the output dataframe\n",
    " significance => entry point - tells spark you're just about to save this thing. Without it, you're just staring at a DataFrame in memory doing nothing. \n",
    "\n",
    " - .format(\"csv)\n",
    "   - What: Sets the output format to csv\n",
    "   - Significance - Defines how the data's structured on disk. Spark supports lots of formats\n",
    " - mode(\"overwrite)\n",
    "  - Specifies what happens if the destination already exists - here it overwrites it\n",
    "\n",
    " - Significance => Controls overwrite vs. append behaviour. Options:\n",
    "   - overwrite: nukes existing data, writes new stuff\n",
    "   - append: adds to existing data\n",
    "   - ignore: skips writing if the path exists\n",
    "   - error: throws an error if the path exists\n",
    " \n",
    "- .option(\"path\",\"file.......\")\n",
    " - \n",
    "- .partitionBy(\"age\") - splits the output into subdirectories based on the age column\n",
    "Organises the data for performance and usability - \n",
    "Assumes age is key for later queries or organisation\n",
    "\n",
    ".save() => triggers the write operation with all the above settings\n",
    "\n",
    "\n",
    "Write Format Options:\n",
    " - CSV => Plain text, comma separated, human readable, works with excel, universal compatibility\n",
    " Cons - no compression, slow to read, no schema enforcement\n",
    " when: small data, sharing with non-technical people, or legacy systems\n",
    "\n",
    " - Parquet\n",
    " Columnar storage - optimised for big data\n",
    " Pros - fast reads, compression, stores schema, spark/hive friendly\n",
    " Cons - Not human-readable, needs tools to open\n",
    " When: Big data, performance matters, querying subsets \n",
    "\n",
    " ORC - Optimised row columnar\n",
    "\n",
    " - another columnar-format, hive optimised\n",
    " - pros - like parquet-compression, fast reads, indexing\n",
    " - cons - less spark-native than parquet, similar trade-offs\n",
    "\n",
    " JSON - Nested text-based format\n",
    " Nested text-based format\n",
    " Pros - Flexible, readable, good for semi-structured data\n",
    " Cons - Bloated, slow, no columnar benefits\n",
    " when: APIs, small datasets, or nested data needs\n",
    "\n",
    " Avro:\n",
    "  Row based, schema-evolving format\n",
    "  pros - compact, schema evolution, \n",
    "  cons - less columnar efficiency, not as spark optimised\n",
    "  when - data pipelines needing schema changes over time\n",
    "\n",
    "Why CSV Here - Your example \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.write\\\n",
    "      .format(\"parquet\")\\\n",
    "      .mode(\"overwrite\")\\\n",
    "      .option(\"path\",r\"C:\\Users\\blais\\Documents\\data_engineering\\week5\\apache_spark\\write_outputs\")\\\n",
    "      .partitionBy(\"age\")\\\n",
    "      .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How then can you read this data back to disk?:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx = spark.read.parquet(r\"C:\\Users\\blais\\Documents\\data_engineering\\week5\\apache_spark\\write_outputs\").where(\"age=25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-------+--------------------+---+\n",
      "|userID|   name|friends|           insert_ts|age|\n",
      "+------+-------+-------+--------------------+---+\n",
      "|   166|Lwaxana|     10|2025-04-05 11:07:...| 25|\n",
      "|   464|Beverly|    485|2025-04-05 11:07:...| 25|\n",
      "|    24| Julian|      1|2025-04-05 11:07:...| 25|\n",
      "|   238| Deanna|    305|2025-04-05 11:07:...| 25|\n",
      "|   315| Weyoun|    208|2025-04-05 11:07:...| 25|\n",
      "|   108|  Leeta|    274|2025-04-05 11:07:...| 25|\n",
      "|    46|   Morn|     96|2025-04-05 11:07:...| 25|\n",
      "|    96|   Ezri|    233|2025-04-05 11:07:...| 25|\n",
      "|   112|   Morn|     13|2025-04-05 11:07:...| 25|\n",
      "|   242|   Data|    101|2025-04-05 11:07:...| 25|\n",
      "|   271|   Morn|    446|2025-04-05 11:07:...| 25|\n",
      "+------+-------+-------+--------------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfx.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can call the where clause on your read to make it cheaper and faster since the written data is already partitioned.\n",
    " - PART 1: WRITING DATA - All the ways you might do it\n",
    "\n",
    " Option B: Write as parquet, partitioned by 2 columns - age and friends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.write\\\n",
    "      .format(\"parquet\")\\\n",
    "      .mode(\"overwrite\")\\\n",
    "      .option(\"path\",r\"C:\\Users\\blais\\Documents\\data_engineering\\week5\\apache_spark\\write_outputs2\")\\\n",
    "      .partitionBy(\"age\",\"friends\")\\\n",
    "      .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx2 = spark.read.parquet(r\"C:\\Users\\blais\\Documents\\data_engineering\\week5\\apache_spark\\write_outputs2\").where(\"friends > 400\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+--------------------+---+-------+\n",
      "|userID|   name|           insert_ts|age|friends|\n",
      "+------+-------+--------------------+---+-------+\n",
      "|   464|Beverly|2025-04-05 11:07:...| 25|    485|\n",
      "|   399|Beverly|2025-04-05 11:07:...| 24|    401|\n",
      "|   106|Beverly|2025-04-05 11:07:...| 18|    499|\n",
      "|   377|Beverly|2025-04-05 11:07:...| 18|    418|\n",
      "|   200| Kasidy|2025-04-05 11:07:...| 21|    472|\n",
      "|   265| Gowron|2025-04-05 11:07:...| 27|    471|\n",
      "|    66| Geordi|2025-04-05 11:07:...| 21|    477|\n",
      "|   280|  Nerys|2025-04-05 11:07:...| 26|    492|\n",
      "|   484|  Leeta|2025-04-05 11:07:...| 22|    478|\n",
      "|   244|  Dukat|2025-04-05 11:07:...| 21|    471|\n",
      "|   444|  Keiko|2025-04-05 11:07:...| 18|    472|\n",
      "|   206|   Will|2025-04-05 11:07:...| 21|    491|\n",
      "|   439|   Data|2025-04-05 11:07:...| 18|    417|\n",
      "|   271|   Morn|2025-04-05 11:07:...| 25|    446|\n",
      "|   304|   Will|2025-04-05 11:07:...| 19|    404|\n",
      "|    89|   Worf|2025-04-05 11:07:...| 24|    492|\n",
      "|    25|    Ben|2025-04-05 11:07:...| 21|    445|\n",
      "+------+-------+--------------------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfx2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
